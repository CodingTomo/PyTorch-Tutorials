{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PyTorch - Calculus.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyN371xDfIFtrkou6xhGtAYD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CodingTomo/PyTorch-Tutorials/blob/master/PyTorch_Calculus.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2xzJ-_wrAO_J",
        "colab_type": "text"
      },
      "source": [
        "### Calculus\n",
        "\n",
        "Pytorch implementa alcune funzioni per il calcolo differenziale.\n",
        "\n",
        "**Automatic differentation** (o *autodiff*) è la caratteristica principale del framework e permette di differenziare qualsiasi funzione rispetto ai suoi input in modo automatico. In pratica consente al programmatore di non preoccuparsi del calcolo del gradienti che in molti algoritmi di ottimizzazione si rendono necessari. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YsJo86YVqi7R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "\n",
        "if not os.path.exists('/content/PyTorch-Tutorials'):\n",
        "  !git clone https://github.com/CodingTomo/PyTorch-Tutorials #clono la repository"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jbna96ih8aua",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fqTsCM9qAM5c",
        "colab_type": "text"
      },
      "source": [
        "Consideriamo la funzione: $f(x) = 3 x^2 + 4$. La sua derivata vale $f^\\prime(x)=6 x$. Calcoliamola struttando PyTorch."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1MsMEmH4gN9g",
        "colab_type": "text"
      },
      "source": [
        "**Osservazione**: ogni tensore ha un flag *requires_grad*. Questo flag permette di anticipare se il tensore sarà parte o meno del calcolo di un qualche gradiente.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5xdbtlbsdnD7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# il tensore x gioca il ruolo dell'indeterminata. Non potendo gestire il calcolo simbolico, inizializiamo x a 2.0\n",
        "x = torch.tensor(2.0, requires_grad=True) \n",
        "print(\"x = {}\".format(x))\n",
        "\n",
        "# definiamo f(x)\n",
        "y = 3 * x**2  + 4\n",
        "print(\"y = {}\".format(y))\n",
        "\n",
        "# backward() calcola la derivata di una funzione rispetto a tutte le variabili e che hanno il flag 'requires_grad'=True. \n",
        "y.backward()\n",
        "\n",
        "# In questo caso y.backward() va a scrivere nell'attributo x.grad il valore della derivata di y in x=2\n",
        "computed_gradient = x.grad\n",
        "print(\"Il valore calcolato per la derivata è {}\".format(computed_gradient))\n",
        "print(\"Il valore atteso per la derivata è {}\".format(6 * x))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N-aFc_cgWws7",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "Tutta la magia e la potenza di *autodiff* sta nel implemetare la **regola della catena** usata in matematica per calcolare le derivate di funzioni composte.\n",
        "\n",
        "[Ulteriori infomazioni sulla regola della catena](https://en.wikipedia.org/wiki/Chain_rule)\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "Per derivare semplici funzioni, come $3x^2 + 4$, esistono delle regole base da seguire che sono conseguenza dello svolgimento di alcuni particolari limiti di rapporti incrementali. Queste regole base vengono concatenate e usate da PyTorch per ottenere derivate di funzioni più elaborate.\n",
        "\n",
        "**Esempio**:\n",
        "\n",
        "$$y = f(x) = \\text{plus4}(\\text{times3}(\\text{square}(x))),$$\n",
        "\n",
        "la derivata di $y$ rispetto a $x$ può essere calcolata usando la regola della catena:\n",
        "\n",
        "$$f'(x) = \\text{plus4}'(\\cdots) \\,\\cdot\\, \\text{times3}'(\\cdots) \\,\\cdot\\, \\text{square}'(x).$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XUA59loBRIrs",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "Quando indiachiamo `require_grad=True` stiamo dicendo che avremo bisogno di calcolare un qualche gradiente rispetto alla variabile che stiamo istanziando. Da questo momento in poi, PyTorch terrà traccia di ogni operazione che dipende da questa variabile. L'intera storia con cui ricostruire le varie derivate è contenuta in quello che si chiama **computational graph**. Nel nostro caso:\n",
        "\n",
        "<p>\n",
        "<img src=https://github.com/CodingTomo/PyTorch-Tutorials/blob/master/Immagini/cg_1.png?raw=1 width=\"550\" style=\"margin-left: auto;margin-right: auto;display: block;\" />\n",
        "</p>\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mttjutKaXMcu",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "Dopo aver calcolato la funzione obiettivo $y=3x^2+4$, è possibile chiamare il metodo `y.backward()` per calcolare le derivate di $y$ rispetto a tutte le variabili per cui l'attributo `require_grad` è True. Sempre nel nostro caso:\n",
        "\n",
        "<p>\n",
        "<img src=https://github.com/CodingTomo/PyTorch-Tutorials/blob/master/Immagini/cg_2.png?raw=1 width=\"550\" style=\"margin-left: auto;margin-right: auto;display: block;\" />\n",
        "</p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J1xFUABhaVK4",
        "colab_type": "text"
      },
      "source": [
        "Osserviamo che i gradienti si **accumulano**. Bisogna quindi fare attenzione inizializzali quando necessario."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vd29NuHJd5vx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Initialize x with some value\n",
        "x = torch.tensor(2.0, requires_grad=True)\n",
        "y = 3 * x**2  + 4\n",
        "\n",
        "y.backward()\n",
        "print(\"Il valore calcolato per la derivata è {}\".format(x.grad))\n",
        "print(\"Il valore atteso per la derivata è {}\".format(6 * x))\n",
        "\n",
        "print('-'*70)\n",
        "\n",
        "y = 3 * x**2  + 4\n",
        "y.backward()\n",
        "print(\"Il valore calcolato per la derivata è {}\".format(x.grad))\n",
        "print(\"Il valore atteso per la derivata è {}\".format(6 * x))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b-dQDVtV8UeG",
        "colab_type": "text"
      },
      "source": [
        "Con il metodo `detach()` possiamo **interrompere** la catena che tiene traccia di dipendenze dalle variabili già definite."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k6AcXn5q3IyJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "A = torch.rand(1,2, requires_grad=True)\n",
        "B = A.mean()\n",
        "\n",
        "print(\"B : \", B)\n",
        "print(\"B.requires_grad :\", B.requires_grad)\n",
        "\n",
        "C = B.detach()\n",
        "\n",
        "print(\"C : \", C)\n",
        "print(\"C.requires_grad :\", C.requires_grad)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1lwtia8KLGNL",
        "colab_type": "text"
      },
      "source": [
        "**Esempio** *(retta tangente)* : Vogliamo trovare la retta tangente in un punto fissato ad una funzione espressa in coordinate polari della forma $f:[0,\\infty) \\times \\mathbb{R} \\rightarrow \\mathbb{R}$ tale che $(\\rho,\\theta)\\mapsto \\rho - 2\\sin(\\theta)$.\n",
        "\n",
        "\n",
        "\n",
        "1. Generiamo una griglia di punti utili a disegnare la funzione. Questo si può facilmente ottenere usando la funzione *meshgrid()*.\n",
        "2. Passiamo i punti così generati in coordinate polari e calcoliamo il valore della funzione.\n",
        "3. Calcoliamo il gradiente nel punto fissato e usiamolo per generare la retta tangente.\n",
        "\n",
        "Infine, ricordiamo che:\n",
        "\n",
        "$$\n",
        "\\bigg \\{\n",
        "\\begin{array}{rl}\n",
        "\\rho = \\sqrt{x^2+y^2} \\\\\n",
        "\\theta = \\arctan\\big(\\frac{y}{x}\\big) \\\\\n",
        "\\end{array}\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ONFIIOms9NLp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from matplotlib import pyplot as plt\n",
        "\n",
        "def f(x, y):\n",
        "    radius = torch.sqrt(x**2 + y**2)\n",
        "    angle = torch.atan2(y, x)\n",
        "    return radius-(2*torch.sin(angle))\n",
        "\n",
        "\n",
        "def contour_plot():\n",
        "    grid_x, grid_y = torch.meshgrid(torch.arange(-3, 3.1, 0.1), torch.arange(-3, 3.1, 0.1))\n",
        "    outputs = f(grid_x, grid_y)\n",
        "    \n",
        "    plt.contourf(grid_x, grid_y, outputs, 15)\n",
        "    plt.colorbar(label=\"f(x, y)\")\n",
        "    plt.xlabel(\"input x\")\n",
        "    plt.ylabel(\"input y\")\n",
        "\n",
        "contour_plot()\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2gc1fV7T9Pmf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x = torch.tensor(1.0, requires_grad=True)\n",
        "y = torch.tensor(2.0, requires_grad=True)\n",
        "\n",
        "z = f(x, y).backward()\n",
        "\n",
        "df_dx = x.grad\n",
        "df_dy = y.grad\n",
        "\n",
        "def plot_tangent_line(x, y, df_dx, df_dy):\n",
        "    x_points = torch.linspace(-3, 3, 100)\n",
        "    y_points = - df_dx / df_dy * (x_points - x.detach()) + y.detach()\n",
        "    plt.scatter(x.detach(), y.detach(), color='white')\n",
        "    plt.plot(x_points, y_points, color='white')\n",
        "    plt.xlim([-3, 3])\n",
        "    plt.ylim([-3, 3])\n",
        "\n",
        "    \n",
        "contour_plot()\n",
        "plot_tangent_line(x, y, df_dx, df_dy)\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m8WQuM8mcoOG",
        "colab_type": "text"
      },
      "source": [
        "**Ossevazione**: il passaggio nel quale scriviamo la formula della retta tangente non è innoquo e fa uso del teorema della funzione implicita."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f_CuNIMjflfD",
        "colab_type": "text"
      },
      "source": [
        "Autograd distingue due tipi di nodi all'interno del grafo dove memorizza la computazione. \n",
        "\n",
        "\n",
        "*   Le **foglie** sono i tensori definiti dal programmatore e su cui non è stata fatta alcuna operazione.\n",
        "*   I **nodi** sono i tensori risultato di operazioni che possono anche, ma non necessariamente, coinvolgere altri tensori.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v0BmBPLo9clV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "A = torch.tensor([[1., 2.], [3., 4.]], requires_grad=True)\n",
        "B = torch.tensor([[1., 2.], [3., 4.]], requires_grad=True) + 2 \n",
        "C = 5 * A \n",
        "print(\"A.is_leaf :\", A.is_leaf)\n",
        "print(\"B.is_leaf :\", B.is_leaf)\n",
        "print(\"C.is_leaf :\", C.is_leaf)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ksc3w617g7Dx",
        "colab_type": "text"
      },
      "source": [
        "La distinzione è utile da sapere in quanto Pytorch di default memorizza solo i gradienti dell'output rispetto alle foglie. Il risultato è memorizzato in `nome_foglia.grad`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-McWVDb7gM3Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "A = torch.Tensor([[1, 2], [3, 4]])\n",
        "A.requires_grad_()\n",
        "\n",
        "B = 5 * (A + 3)\n",
        "C = B.mean()\n",
        "\n",
        "print(\"A.grad :\", A.grad)\n",
        "print(\"B.grad :\", B.grad)\n",
        "C.backward()\n",
        "print(\"\\n-- Backward --\\n\")\n",
        "print(\"A.grad :\", A.grad)\n",
        "print(\"B.grad :\", B.grad)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NQc4xQbahfg-",
        "colab_type": "text"
      },
      "source": [
        "Nella pratica spesso non è necessario, ma è comunque possibile memorizzare tutti i gradienti di tutti gli step usando la funzione `retain_grad()`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Po8ogs-ChVZg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "A = torch.Tensor([[1, 2], [3, 4]])\n",
        "A.requires_grad_()\n",
        "\n",
        "B = 5 * (A + 3)\n",
        "B.retain_grad()\n",
        "C = B.mean()\n",
        "\n",
        "\n",
        "print(\"A.grad :\", A.grad)\n",
        "print(\"B.grad :\", B.grad)\n",
        "C.backward()\n",
        "print(\"\\n-- Backward --\\n\")\n",
        "print(\"A.grad :\", A.grad)\n",
        "print(\"B.grad :\", B.grad)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}